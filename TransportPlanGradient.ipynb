{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geomloss\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ot\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import seaborn\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "from utils.fix_for_geomloss import sinkhorn_loop\n",
    "\n",
    "geomloss.sinkhorn_samples.sinkhorn_loop = sinkhorn_loop\n",
    "# This is to fix the missing gradient for weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$X \\sim \\sum_i w_i, \\delta_{X_i}$ where the $X_i$'s are i.i.d. $U([-2, 2])$ and $w_i \\propto f(X_i)$ where $f$ is the pdf of a normal distribution $ \\mathcal{N}(0, \\theta ^ 2)$ where $\\theta \\in \\mathbb{R}^+$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = torch.random.manual_seed(31415926)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transport_from_potentials(x, f, g, eps, w, N):\n",
    "    C = (x.T - x) ** 2 / 2.\n",
    "    FG = f.T + g\n",
    "    T = torch.exp((FG - C)/eps**2) * w.unsqueeze(1)\n",
    "    return T.T @ x, torch.full_like(f, 1/N).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_mean_and_grad(N, loc_val, times=10, seed = 27182):\n",
    "#     _ = torch.random.manual_seed(seed)\n",
    "#     res = []\n",
    "\n",
    "#     loc = torch.tensor(loc_val, requires_grad=True)\n",
    "#     scale = 1.\n",
    "#     norm_dist = torch.distributions.Normal(loc, scale)\n",
    "    \n",
    "#     for _ in range(times):\n",
    "#         X = torch.rand(N, requires_grad=True) * 4 - 2.\n",
    "\n",
    "\n",
    "#         weights = norm_dist.log_prob(X)\n",
    "#         max_weight = weights.max()\n",
    "#         stable_weights = (weights - max_weight).exp()\n",
    "#         scaled_weights = stable_weights / stable_weights.sum()\n",
    "#         uniform_weights = torch.full_like(scaled_weights, 1/N, requires_grad=True)\n",
    "\n",
    "#         epsilon = 0.01\n",
    "#         biasedSampleLoss = geomloss.SamplesLoss(reach=None, potentials=True, debias=False, scaling=0.9, blur=epsilon)\n",
    "#         alpha, beta = biasedSampleLoss(uniform_weights, X.unsqueeze(1), scaled_weights, X.unsqueeze(1))\n",
    "#         X_tilde, w_tilde = transport_from_potentials(X.unsqueeze(1), alpha, beta, epsilon, scaled_weights, N)\n",
    "#         res.append((X_tilde.mean(), torch.autograd.grad(X_tilde.mean(), [loc])[0], np.average(X.detach().numpy(), weights=scaled_weights.detach().numpy())))\n",
    "        \n",
    "#     return res\n",
    "\n",
    "def get_grad(N, loc_val, scale_val, seed = 27182):\n",
    "    _ = torch.random.manual_seed(seed)\n",
    "    res = []\n",
    "\n",
    "    loc = torch.tensor(loc_val, requires_grad=True)\n",
    "    scale = scale_val\n",
    "    norm_dist = torch.distributions.Normal(loc, scale)\n",
    "\n",
    "    X = torch.rand(N, requires_grad=True) * 4 - 2.\n",
    "    weights = norm_dist.log_prob(X)\n",
    "    max_weight = weights.max()\n",
    "    stable_weights = (weights - max_weight).exp()\n",
    "    scaled_weights = stable_weights / stable_weights.sum()\n",
    "    uniform_weights = torch.full_like(scaled_weights, 1/N, requires_grad=True)\n",
    "\n",
    "    epsilon = 0.01\n",
    "    biasedSampleLoss = geomloss.SamplesLoss(reach=None, potentials=True, debias=False, scaling=0.9, blur=epsilon)\n",
    "    alpha, beta = biasedSampleLoss(uniform_weights, X.unsqueeze(1), scaled_weights, X.unsqueeze(1))\n",
    "    X_tilde, w_tilde = transport_from_potentials(X.unsqueeze(1), alpha, beta, epsilon, scaled_weights, N)\n",
    "        \n",
    "    return torch.autograd.grad(X_tilde.mean(), [loc])[0].detach().numpy().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_int = np.random.randint(1, 1e6, )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m/\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m      <no docstring>\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\programdata\\anaconda3\\lib\\site-packages\\torch\\__init__.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     Parameter\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "finite_diff = []\n",
    "auto_diff = []\n",
    "\n",
    "seed = random_int \n",
    "grad = get_grad(200, 0., 0.2, seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad_numerically(loc, scale):\n",
    "    linspace = np.linspace(-2, 2, 100)\n",
    "    weights = st.norm.pdf(linspace, loc, scale)\n",
    "    tab = linspace * (linspace - loc) / (scale ** 2) * weights\n",
    "    return np.mean(tab)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_grad_diff(loc, scale, eps=1e-4):\n",
    "    linspace = np.random.uniform(-2, 2, 10000)\n",
    "    weights1 = st.norm.pdf(linspace, loc, scale)\n",
    "    avg_1 = np.average(linspace, weights=weights1)\n",
    "    weights2 = st.norm.pdf(linspace, loc + eps, scale)\n",
    "    avg_2 = np.average(linspace, weights=weights2)\n",
    "    return (avg_2 - avg_1) / eps\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "locs = [-0.5, -0.25, 0., 0.25, 0.5]\n",
    "scales = [0.25, 0.5, 1., 1.5, 2.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "theoretical_gradients = [ [ compute_grad_numerically(loc, scale) for loc in locs ] for scale in scales ]\n",
    "autodiff_gradients = [ [ get_grad(500, loc, scale ) for loc in locs ] for scale in scales ]\n",
    "difference_gradients = [ [ compute_grad_diff(loc, scale) for loc in locs ] for scale in scales ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.24749999, 0.2475    , 0.2475    , 0.2475    , 0.24749999],\n",
       "       [0.24329162, 0.24668183, 0.24726015, 0.24668183, 0.24329162],\n",
       "       [0.15901249, 0.17835222, 0.18493272, 0.17835222, 0.15901249],\n",
       "       [0.08554178, 0.0933635 , 0.09604907, 0.0933635 , 0.08554178],\n",
       "       [0.04666373, 0.04945435, 0.05040407, 0.04945435, 0.04666373]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(theoretical_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.01604342, 1.00435115, 1.03216065, 0.99592444, 0.99907274],\n",
       "       [0.98799672, 0.98150207, 1.00129879, 0.98639051, 0.99169505],\n",
       "       [0.72376156, 0.760581  , 0.78562283, 0.76529858, 0.72282705],\n",
       "       [0.4600503 , 0.45548071, 0.46843671, 0.46043591, 0.45868325],\n",
       "       [0.2919079 , 0.28995706, 0.28853433, 0.2875588 , 0.29051462]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(difference_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "autodiff_gradients = pd.DataFrame(np.array(autodiff_gradients), columns = locs, index = scales )\n",
    "theoretical_gradients = pd.DataFrame(np.array(theoretical_gradients), columns = locs, index = scales )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "{} &     -0.50 &     -0.25 &      0.00 &      0.25 &      0.50 \\\\\n",
      "\\midrule\n",
      "0.25 &  0.053832 &  0.116715 &  0.145680 &  0.124257 &  0.039876 \\\\\n",
      "0.50 &  0.143278 &  0.228782 &  0.259055 &  0.245363 &  0.164323 \\\\\n",
      "1.00 &  0.116835 &  0.169257 &  0.189752 &  0.167950 &  0.114907 \\\\\n",
      "1.50 &  0.049181 &  0.050599 &  0.051048 &  0.049108 &  0.046073 \\\\\n",
      "2.00 &  0.017662 &  0.017207 &  0.016862 &  0.016544 &  0.016984 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "{} &     -0.50 &     -0.25 &      0.00 &      0.25 &      0.50 \\\\\n",
      "\\midrule\n",
      "0.25 &  0.247500 &  0.247500 &  0.247500 &  0.247500 &  0.247500 \\\\\n",
      "0.50 &  0.243292 &  0.246682 &  0.247260 &  0.246682 &  0.243292 \\\\\n",
      "1.00 &  0.159012 &  0.178352 &  0.184933 &  0.178352 &  0.159012 \\\\\n",
      "1.50 &  0.085542 &  0.093363 &  0.096049 &  0.093363 &  0.085542 \\\\\n",
      "2.00 &  0.046664 &  0.049454 &  0.050404 &  0.049454 &  0.046664 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(autodiff_gradients.to_latex())\n",
    "print()\n",
    "print(theoretical_gradients.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-f0eecad78caa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mgrads_plus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'mean'"
     ]
    }
   ],
   "source": [
    "grads_plus.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = np.array([ k.detach().numpy().sum() for k in grads ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  1., 75., 17.,  0.,  2.,  2.,  0.,  0.,  2.]),\n",
       " array([-3.3661075 , -2.504196  , -1.6422846 , -0.78037316,  0.0815383 ,\n",
       "         0.94344974,  1.8053612 ,  2.6672726 ,  3.529184  ,  4.3910956 ,\n",
       "         5.253007  ], dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(diff_grad / grads - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'diff_grad' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-b97145c8f9bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdiff_grad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'diff_grad' is not defined"
     ]
    }
   ],
   "source": [
    "diff_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04227133,  0.04091082, -0.00978464, -0.09499675,  0.03833718,\n",
       "       -0.02371318, -0.00136001, -0.03693785, -0.12171192, -0.00899773,\n",
       "        0.08610975,  0.03607482,  0.07692624, -0.12898453, -0.04471462,\n",
       "       -0.02538247, -0.07557618, -0.00187918, -0.04974646, -0.06259524,\n",
       "       -0.00230798,  0.06862214, -0.02503491, -0.0661673 , -0.03061059,\n",
       "       -0.05011686,  0.05488196,  0.03896873,  0.0900353 ,  0.00844844,\n",
       "        0.01810246, -0.02338212, -0.02952169,  0.1095877 ,  0.00367017,\n",
       "        0.12743108,  0.04299772, -0.07400817, -0.05347732, -0.07454488,\n",
       "        0.01672689,  0.03989759,  0.00230828,  0.01066031,  0.04254557,\n",
       "       -0.02128766, -0.03349001, -0.07224014,  0.03659174,  0.02908496,\n",
       "        0.05064617,  0.01330398,  0.00302699,  0.00479886, -0.0773503 ,\n",
       "       -0.00085711,  0.0578791 , -0.01675381,  0.06852276, -0.03650057,\n",
       "       -0.07032397,  0.03814838,  0.0409254 , -0.07700247, -0.07394167,\n",
       "       -0.04611317,  0.21193849, -0.00216643, -0.03921271, -0.04092754,\n",
       "        0.05168538,  0.01537215,  0.02457662, -0.02516538, -0.06278306,\n",
       "        0.0197621 ,  0.01705059, -0.16428125,  0.06151508, -0.04696641,\n",
       "       -0.00885852,  0.07771312, -0.03547622, -0.05129365,  0.06806203,\n",
       "        0.02063494, -0.09495102,  0.0762118 , -0.07041769,  0.06219757,\n",
       "       -0.04203811,  0.03276083,  0.04804731, -0.10035471, -0.01559306,\n",
       "        0.07039989,  0.06394929,  0.04769772, -0.03925987, -0.05998191],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
